from graph import create_alpha_flow_graph
from tools.stock_data import search_stock_code, get_cache_status
from dotenv import load_dotenv
import os
import sys
import json
from datetime import datetime, timedelta
from pathlib import Path

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸º UTF-8ï¼Œé˜²æ­¢ Windows ä¸‹ emoji å¯¼è‡´å´©æºƒ
def setup_utf8_encoding():
    """
    åœ¨ Windows ç³»ç»Ÿä¸Šè®¾ç½® UTF-8 ç¼–ç ï¼Œæ”¯æŒ emoji å­—ç¬¦
    """
    if sys.platform == 'win32':
        try:
            # å°è¯•ä½¿ç”¨ reconfigure æ–¹æ³• (Python 3.7+)
            if hasattr(sys.stdout, 'reconfigure'):
                sys.stdout.reconfigure(encoding='utf-8')
                sys.stderr.reconfigure(encoding='utf-8')
            else:
                # Python < 3.7 çš„å…¼å®¹å¤„ç†
                import io
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
                sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
        except Exception as e:
            # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šé‡åˆ°ç¼–ç é—®é¢˜
            pass

setup_utf8_encoding()

# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå·²è®¾ç½®çš„ç¯å¢ƒå˜é‡ (override=False)
load_dotenv(override=False)

# æ¨¡å‹æ¢æµ‹ç¼“å­˜æ–‡ä»¶è·¯å¾„
MODEL_CACHE_FILE = Path(__file__).parent / ".model_cache.json"

def load_model_cache():
    """
    åŠ è½½æ¨¡å‹æ¢æµ‹ç¼“å­˜
    å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œè¿”å› None
    """
    try:
        if not MODEL_CACHE_FILE.exists():
            return None
        
        with open(MODEL_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
        cache_time = datetime.fromisoformat(cache_data.get("cache_time", ""))
        if datetime.now() - cache_time > timedelta(hours=24):
            print("ğŸ“¦ æ¨¡å‹æ¢æµ‹ç¼“å­˜å·²è¿‡æœŸ")
            return None
        
        print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹æ¢æµ‹ç¼“å­˜: {cache_data.get('model_name', 'unknown')}")
        return cache_data.get("model_name")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æ¨¡å‹ç¼“å­˜å¤±è´¥: {e}")
        return None

def save_model_cache(model_name: str):
    """
    ä¿å­˜æ¨¡å‹æ¢æµ‹ç»“æœåˆ°ç¼“å­˜æ–‡ä»¶
    """
    try:
        cache_data = {
            "model_name": model_name,
            "cache_time": datetime.now().isoformat()
        }
        with open(MODEL_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ¨¡å‹æ¢æµ‹ç»“æœå·²ç¼“å­˜: {model_name}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¨¡å‹ç¼“å­˜å¤±è´¥: {e}")

def detect_available_model(api_key: str, api_base: str, force_redetect: bool = False):
    """
    è‡ªåŠ¨æ¢æµ‹å¯ç”¨çš„æ¨¡å‹
    è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚æœéƒ½ä¸å¯ç”¨åˆ™è¿”å› None
    
    Args:
        api_key: API Key
        api_base: API Base URL
        force_redetect: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ¢æµ‹ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    """
    # å¦‚æœä¸å¼ºåˆ¶é‡æ–°æ¢æµ‹ï¼Œå…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
    if not force_redetect:
        cached_model = load_model_cache()
        if cached_model:
            return cached_model
    
    from langchain_openai import ChatOpenAI
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    supported_models_str = os.getenv("SUPPORTED_MODELS", "")
    if supported_models_str:
        supported_models = [m.strip() for m in supported_models_str.split(",")]
    else:
        # é»˜è®¤æ¨¡å‹åˆ—è¡¨
        supported_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "mimo-v2-flash"]
    
    print(f"ğŸ” å¼€å§‹æ¢æµ‹å¯ç”¨æ¨¡å‹ï¼Œå€™é€‰åˆ—è¡¨: {', '.join(supported_models)}")
    
    for model_name in supported_models:
        try:
            print(f"  å°è¯•æ¨¡å‹: {model_name}...")
            llm = ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=5,
                timeout=10
            )
            llm.invoke("hi")
            print(f"  âœ… æ¨¡å‹ {model_name} å¯ç”¨")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            save_model_cache(model_name)
            
            return model_name
        except Exception as e:
            print(f"  âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨: {str(e)[:50]}")
            continue
    
    print("âŒ æ‰€æœ‰å€™é€‰æ¨¡å‹å‡ä¸å¯ç”¨")
    return None

def run_alpha_flow(input_str: str):
    """
    è¿è¡Œ AlphaFlow æŠ•èµ„å†³ç­–ç³»ç»Ÿ
    input_str: å¯ä»¥æ˜¯è‚¡ç¥¨ä»£ç  (å¦‚ 600519) æˆ–è‚¡ç¥¨åç§° (å¦‚ è´µå·èŒ…å°)
    """
    # æ£€æŸ¥å¹¶è·å– API Key
    api_key = os.getenv("OPENAI_API_KEY")
    api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    # ä¼˜å…ˆè·å–ç”¨æˆ·åœ¨ç¯å¢ƒå˜é‡ä¸­æ˜¾å¼æŒ‡å®šçš„æ¨¡å‹åç§°
    model_name = os.getenv("MODEL_NAME") or os.getenv("OPENAI_MODEL_NAME")

    if not api_key or api_key == "your_openai_api_key":
        print("âš ï¸ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æœ‰æ•ˆçš„ OPENAI_API_KEY")
        return

    # æ¨¡å‹å¯ç”¨æ€§é¢„æ£€
    print(f"\nğŸ§ª æ¨¡å‹å¯ç”¨æ€§é¢„æ£€...")
    
    # å¦‚æœç¯å¢ƒå˜é‡å·²ç»æŒ‡å®šäº†æ¨¡å‹ï¼Œå…ˆå°è¯•ä½¿ç”¨è¯¥æ¨¡å‹
    available_model = None
    if model_name:
        print(f"  å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ¨¡å‹: {model_name}...")
        from langchain_openai import ChatOpenAI
        try:
            llm = ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=5,
                timeout=10
            )
            llm.invoke("hi")
            available_model = model_name
            print(f"  âœ… æŒ‡å®šæ¨¡å‹ {model_name} å¯ç”¨")
        except Exception as e:
            print(f"  âŒ æŒ‡å®šæ¨¡å‹ {model_name} ä¸å¯ç”¨ï¼Œå°†å°è¯•è‡ªåŠ¨æ¢æµ‹å…¶ä»–å¯ç”¨æ¨¡å‹...")
    
    # å¦‚æœæŒ‡å®šæ¨¡å‹ä¸å¯ç”¨æˆ–æœªæŒ‡å®šï¼Œåˆ™è¿›è¡Œè‡ªåŠ¨æ¢æµ‹
    if not available_model:
        available_model = detect_available_model(api_key, api_base)
    
    if not available_model:
        print("\nâŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®")
        print("   2. æ£€æŸ¥ API Base URL æ˜¯å¦æ­£ç¡®")
        print("   3. åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPPORTED_MODELSï¼Œåˆ—å‡ºæ‚¨çš„ API æœåŠ¡å•†æ”¯æŒçš„æ¨¡å‹")
        print("   4. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        print("   5. å¦‚æœä½¿ç”¨ä»£ç†ï¼Œè¯·æ£€æŸ¥ä»£ç†è®¾ç½®")
        return
    
    # ä½¿ç”¨æ¢æµ‹åˆ°çš„å¯ç”¨æ¨¡å‹
    model_name = available_model
    print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_name}\n")

    # è¯†åˆ«è¾“å…¥æ˜¯ä»£ç è¿˜æ˜¯åç§°
    stock_code = ""
    stock_name = ""
    
    if input_str.isdigit() and len(input_str) == 6:
        stock_code = input_str
        stock_name = input_str # ç¨åå¯ä»¥åœ¨èŠ‚ç‚¹ä¸­è¿›ä¸€æ­¥å®Œå–„
    else:
        print(f"ğŸ” æ­£åœ¨æœç´¢è‚¡ç¥¨ä»£ç : {input_str}...")
        stock_code, stock_name = search_stock_code(input_str)
        if not stock_code:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨: {input_str}")
            return
        print(f"âœ… å·²æ‰¾åˆ°: {stock_name} ({stock_code})")

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "stock_code": stock_code,
        "stock_name": stock_name,
        "news_items": [],
        "news_analysis": "",
        "sentiment_score": 0.0,
        "quant_data": {},
        "technical_indicators": {},
        "strategy_report": "",
        "risk_assessment": "",
        "messages": [],
        "revision_needed": False,
        "human_approval": False,
        "count": 0,
        "is_sector": False,
        "error": "",
        "config": {
            "api_key": api_key,
            "api_base": api_base,
            "model_name": model_name,
            "temperature": 0.3,
            "max_tokens": 8196,
            "thinking_mode": True
        }
    }
    
    # åˆ›å»ºå¹¶è¿è¡Œå›¾
    app = create_alpha_flow_graph()
    
    print(f"\nğŸš€ AlphaFlow å¯åŠ¨: æ­£åœ¨åˆ†æè‚¡ç¥¨ {stock_code} ({stock_name})...\n")
    
    # è¿è¡Œ
    try:
        # ä½¿ç”¨ stream æ¨¡å¼ä»¥ä¾¿åœ¨èŠ‚ç‚¹å‡ºé”™æ—¶åŠæ—¶å‘ç°
        final_state = initial_state
        for output in app.stream(initial_state):
            for node_name, state_update in output.items():
                final_state.update(state_update)
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å‘ç”Ÿ
                if final_state.get("error"):
                    print(f"\nğŸ›‘ æµç¨‹å› èŠ‚ç‚¹é”™è¯¯ä¸­æ­¢: {final_state['error']}")
                    print("ğŸ’¡ å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆ:")
                    print("   - æ¨¡å‹ä¸æ”¯æŒ: è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPPORTED_MODELS")
                    print("   - API Key æ— æ•ˆ: è¯·æ£€æŸ¥ OPENAI_API_KEY æ˜¯å¦æ­£ç¡®")
                    print("   - ç½‘ç»œè¿æ¥é—®é¢˜: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®")
                    print("   - æ•°æ®æºé—®é¢˜: AkShare æ•°æ®æºå¯èƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
                    return
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­ä¿¡å·
                if final_state.get("interrupted"):
                    print(f"\nâ¸ï¸ æµç¨‹è¢«ç”¨æˆ·ä¸­æ–­")
                    return
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        print("\n" + "="*50)
        print("ğŸ“¦ æ•°æ®ç¼“å­˜çŠ¶æ€")
        print("="*50)
        
        cache_status = get_cache_status(stock_code)
        
        if cache_status and cache_status.get("data_sources"):
            print(f"ç¼“å­˜æ–‡ä»¶: {cache_status['cache_file']}")
            print(f"æ€»ç¼“å­˜æ¡ç›®: {cache_status['cache_size']}")
            print("\nå„æ•°æ®æºæœ€åæ›´æ–°æ—¶é—´:")
            
            data_sources = cache_status["data_sources"]
            has_cached_data = False
            for source_name, source_info in data_sources.items():
                last_updated = source_info.get("last_updated")
                if last_updated:
                    try:
                        dt = datetime.fromisoformat(last_updated)
                        time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                        print(f"  - {source_name}: {time_str}")
                        has_cached_data = True
                    except:
                        print(f"  - {source_name}: {last_updated}")
                        has_cached_data = True
                else:
                    print(f"  - {source_name}: æœªç¼“å­˜")
            
            if not has_cached_data:
                print("  (æš‚æ— ç¼“å­˜æ•°æ®)")
        else:
            print("æš‚æ— ç¼“å­˜æ•°æ®")
        
        print("\n" + "="*50)
        print("ğŸ“‹ æœ€ç»ˆæŠ•èµ„å»ºè®®æŠ¥å‘Š")
        print("="*50)
        print(final_state.get("strategy_report", "æœªç”ŸæˆæŠ¥å‘Š"))
        print("\n" + "="*50)
        print("ğŸ›¡ï¸ é£æ§å®¡æ ¸æ„è§")
        print("="*50)
        risk_assessment = final_state.get("risk_assessment", {})
        
        # å¤„ç†ç»“æ„åŒ–çš„é£æ§ç»“æœ
        if isinstance(risk_assessment, dict):
            decision = risk_assessment.get("decision", "æœªçŸ¥")
            reason = risk_assessment.get("reason", "æœªæä¾›è¯¦ç»†ç†ç”±")
            review_count = risk_assessment.get("review_count", 0)
            review_date = risk_assessment.get("review_date", "")
            
            print(f"ã€å†³ç­–: {decision}ã€‘")
            print(f"ã€å®¡æ ¸æ¬¡æ•°: {review_count}ã€‘")
            if review_date:
                print(f"ã€å®¡æ ¸æ—¥æœŸ: {review_date}ã€‘")
            print(f"\n{reason}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼ºåˆ¶é€šè¿‡
            if decision == "å¼ºåˆ¶é€šè¿‡":
                print("\nâš ï¸ **é‡è¦æç¤º**: è¯¥æŠ¥å‘Šå·²è¾¾åˆ°æœ€å¤§ä¿®è®¢æ¬¡æ•°ï¼Œç³»ç»Ÿå¼ºåˆ¶é€šè¿‡ã€‚")
                print("âš ï¸ **é£é™©æç¤º**: è¯·ä»”ç»†é˜…è¯»é£æ§å®˜çš„æœ«æ¬¡é£é™©æç¤ºï¼Œå»ºè®®äººå·¥å¤æ ¸åå†åšæŠ•èµ„å†³ç­–ã€‚")
        else:
            # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
            print(risk_assessment)
            if "å¼ºåˆ¶é€šè¿‡" in str(risk_assessment):
                print("\nâš ï¸ **é‡è¦æç¤º**: è¯¥æŠ¥å‘Šå·²è¾¾åˆ°æœ€å¤§ä¿®è®¢æ¬¡æ•°ï¼Œç³»ç»Ÿå¼ºåˆ¶é€šè¿‡ã€‚")
                print("âš ï¸ **é£é™©æç¤º**: è¯·ä»”ç»†é˜…è¯»é£æ§å®˜çš„æœ«æ¬¡é£é™©æç¤ºï¼Œå»ºè®®äººå·¥å¤æ ¸åå†åšæŠ•èµ„å†³ç­–ã€‚")
        print("="*50)
        print("âœ¨ åˆ†æä»»åŠ¡å®Œæˆ")
    except Exception as e:
        print(f"ğŸ’¥ ç³»ç»Ÿè¿è¡Œå‡ºé”™: {str(e)}")

if __name__ == "__main__":
    import argparse
    
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='AlphaFlow è‚¡ç¥¨åˆ†æç³»ç»Ÿ')
    parser.add_argument('--stock', type=str, help='è‚¡ç¥¨ä»£ç æˆ–åç§° (ä¾‹å¦‚: 600519 æˆ– è´µå·èŒ…å°)')
    
    args = parser.parse_args()
    
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œè¾“å…¥æˆ–ç›´æ¥ä¿®æ”¹æ­¤å¤„
    if args.stock:
        run_alpha_flow(args.stock)
    else:
        user_input = input("è¯·è¾“å…¥è‚¡ç¥¨åç§°æˆ–ä»£ç  (ä¾‹å¦‚: è´µå·èŒ…å° æˆ– 600519): ").strip()
        if user_input:
            run_alpha_flow(user_input)
        else:
            run_alpha_flow("600519") # é»˜è®¤æµ‹è¯•èŒ…å°
