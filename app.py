import streamlit as st
import pandas as pd
import os
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
from graph import create_alpha_flow_graph
from tools.stock_data import search_stock_code, get_stock_hist_data, search_board_info, get_board_hist_data, get_board_cons, get_cache_status
import plotly.graph_objects as go
from pathlib import Path

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AlphaFlow æ™ºèƒ½æŠ•èµ„å†³ç­–ç³»ç»Ÿ",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå·²è®¾ç½®çš„ç¯å¢ƒå˜é‡ (override=False)
load_dotenv(override=False)

# æ¨¡å‹æ¢æµ‹ç¼“å­˜æ–‡ä»¶è·¯å¾„
MODEL_CACHE_FILE = Path(__file__).parent / ".model_cache.json"

def load_model_cache():
    """
    åŠ è½½æ¨¡å‹æ¢æµ‹ç¼“å­˜
    å¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œè¿”å› None
    """
    try:
        if not MODEL_CACHE_FILE.exists():
            return None
        
        with open(MODEL_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
        cache_time = datetime.fromisoformat(cache_data.get("cache_time", ""))
        if datetime.now() - cache_time > timedelta(hours=24):
            return None
        
        return cache_data.get("model_name")
    except Exception as e:
        return None

def save_model_cache(model_name: str):
    """
    ä¿å­˜æ¨¡å‹æ¢æµ‹ç»“æœåˆ°ç¼“å­˜æ–‡ä»¶
    """
    try:
        cache_data = {
            "model_name": model_name,
            "cache_time": datetime.now().isoformat()
        }
        with open(MODEL_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        pass

# åˆå§‹åŒ–å†å²è®°å½•ç›®å½•
HISTORY_DIR = "analysis_history"
if not os.path.exists(HISTORY_DIR):
    os.makedirs(HISTORY_DIR)

def save_history(stock_name, stock_code, report):
    filename = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{stock_code}.json"
    filepath = os.path.join(HISTORY_DIR, filename)
    data = {
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "stock_name": stock_name,
        "stock_code": stock_code,
        "report": report
    }
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def get_history_list():
    if not os.path.exists(HISTORY_DIR):
        return []
    files = [f for f in os.listdir(HISTORY_DIR) if f.endswith(".json")]
    files.sort(reverse=True)
    return files

def delete_history(filename):
    filepath = os.path.join(HISTORY_DIR, filename)
    if os.path.exists(filepath):
        os.remove(filepath)

def clear_all_history():
    if os.path.exists(HISTORY_DIR):
        for f in os.listdir(HISTORY_DIR):
            if f.endswith(".json"):
                os.remove(os.path.join(HISTORY_DIR, f))

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "workflow_state" not in st.session_state:
    st.session_state.workflow_state = None
if "app" not in st.session_state:
    st.session_state.app = create_alpha_flow_graph()
if "current_stock" not in st.session_state:
    st.session_state.current_stock = None

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("âš™ï¸ ç³»ç»Ÿé…ç½®")
    
    # å†å²è®°å½•é€‰é¡¹å¡
    history_tab, config_tab = st.tabs(["ğŸ•’ å†å²è®°å½•", "ğŸ› ï¸ é…ç½®"])
    
    with history_tab:
        history_files = get_history_list()
        if not history_files:
            st.info("æš‚æ— å†å²æŠ¥å‘Š")
        else:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨å†å²", width="stretch"):
                clear_all_history()
                st.rerun()
            st.divider()
            
            for h_file in history_files[:20]: # æ˜¾ç¤ºæœ€è¿‘20ä¸ª
                try:
                    with open(os.path.join(HISTORY_DIR, h_file), "r", encoding="utf-8") as f:
                        h_data = json.load(f)
                        
                        col1, col2 = st.columns([0.8, 0.2])
                        with col1:
                            if st.button(f"{h_data['date']}\n{h_data['stock_name']}", key=f"btn_{h_file}", width="stretch"):
                                st.session_state.messages = [{"role": "assistant", "content": h_data['report']}]
                                st.rerun()
                        with col2:
                            # ä½¿ç”¨ container æ¨¡å¼å¹¶è®¾ç½®æŒ‰é’®å®½åº¦ï¼Œç¡®ä¿å›¾æ ‡å±…ä¸­ä¸”ä¸æº¢å‡º
                            if st.button("âŒ", key=f"del_{h_file}", help="åˆ é™¤æ­¤è®°å½•", width="stretch"):
                                delete_history(h_file)
                                st.rerun()
                except:
                    pass

    with st.expander("ğŸ“Š å›æµ‹å€™é€‰ç­–ç•¥"):
        state = st.session_state.workflow_state or {}
        quant_data = state.get("quant_data", {})
        candidates = quant_data.get("backtest_candidates", [])
        if candidates:
            st.write(f"**æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ç­–ç•¥**")
            for i, cand in enumerate(candidates[:5]): # æ˜¾ç¤ºå‰5ä¸ª
                metrics = cand.get('metrics', {})
                with st.container():
                    col1, col2, col3 = st.columns(3)
                    col1.metric(f"{cand.get('name')}", f"{metrics.get('sharpe', 0):.2f}", "Sharpe")
                    col2.metric("CAGR", f"{metrics.get('cagr', 0)*100:.2f}%")
                    col3.metric("MDD", f"{metrics.get('max_drawdown', 0)*100:.2f}%")
                    if i < len(candidates[:5]) - 1:
                        st.divider()
        else:
            st.info("æš‚æ— å¯ç”¨çš„å€™é€‰ç­–ç•¥æ•°æ®")

    with config_tab:
        # è·å–é»˜è®¤æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
        default_model = os.getenv("MODEL_NAME") or os.getenv("OPENAI_MODEL_NAME") or "gpt-4o"
        common_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "mimo-v2-flash"]
        
        # å¦‚æœæ˜¯ DeepSeek ç­‰è‡ªå®šä¹‰æ¨¡å‹ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        if "deepseek" in default_model.lower() and "deepseek-v3" not in [m.lower() for m in common_models]:
             common_models.insert(0, default_model)
        elif default_model not in common_models:
            common_models.insert(0, default_model)
        
        selected_model = st.selectbox(
            "é€‰æ‹©æˆ–è¾“å…¥åˆ†ææ¨¡å‹",
            common_models,
            index=common_models.index(default_model) if default_model in common_models else 0
        )
        
        custom_model = st.text_input("è‡ªå®šä¹‰æ¨¡å‹åç§° (å¯é€‰)", value=selected_model)
        model_to_use = custom_model if custom_model else selected_model
        
        st.divider()
        st.subheader("ğŸ”‘ API å‡­æ®é…ç½®")
        
        api_base = st.text_input(
            "API Base URL (ä»£ç†åœ°å€)", 
            value=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        )
        api_key = st.text_input(
            "API Key", 
            value=os.getenv("OPENAI_API_KEY", ""), 
            type="password"
        )
        
        if not api_key:
            st.warning("âš ï¸ è¯·è¾“å…¥ API Key ä»¥å¼€å§‹åˆ†æ")
        
        temperature = st.slider("Temperature (éšæœºæ€§)", 0.0, 1.0, 0.3, 0.1)
        max_tokens = st.select_slider("Max Tokens (æœ€å¤§é•¿åº¦)", options=[1024, 2048, 4096, 8192, 8196, 16384, 32768], value=8196)
        
        thinking_mode = st.toggle("å¼€å¯æ·±åº¦æ€è€ƒæ¨¡å¼ (Thinking Mode)", value=True)
        
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å½“å‰å¯¹è¯"):
            st.session_state.messages = []
            st.session_state.workflow_state = None
            st.rerun()

st.title("ğŸ“ˆ AlphaFlow æ™ºèƒ½æŠ•èµ„å†³ç­–ç³»ç»Ÿ")
st.caption("åŸºäº LangGraph çš„å¤šæ™ºèƒ½ä½“åä½œ A è‚¡å†³ç­–å¹³å°")

# å±•ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†å·¥ä½œæµé€»è¾‘
@st.cache_data(ttl=3600, show_spinner=False)
def _get_entity_info(input_str):
    """ç¼“å­˜ç‰ˆå®ä½“ä¿¡æ¯æ£€ç´¢"""
    is_sector = False
    sector_type = ""
    sector_cons = []
    
    if input_str.isdigit() and len(input_str) == 6:
        return input_str, input_str, False, "", []
    
    board_info = search_board_info(input_str)
    if board_info:
        is_sector = True
        stock_code = board_info["code"]
        stock_name = board_info["name"]
        sector_type = board_info["type"]
        sector_cons = get_board_cons(stock_name, sector_type)
        return stock_code, stock_name, is_sector, sector_type, sector_cons
    
    stock_code, stock_name = search_stock_code(input_str)
    return stock_code, stock_name, False, "", []

def detect_available_model_st(api_key: str, api_base: str):
    """
    è‡ªåŠ¨æ¢æµ‹å¯ç”¨çš„æ¨¡å‹ (Streamlit ç‰ˆ)
    è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚æœéƒ½ä¸å¯ç”¨åˆ™è¿”å› None
    """
    from langchain_openai import ChatOpenAI
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    supported_models_str = os.getenv("SUPPORTED_MODELS", "")
    if supported_models_str:
        supported_models = [m.strip() for m in supported_models_str.split(",")]
    else:
        # é»˜è®¤æ¨¡å‹åˆ—è¡¨
        supported_models = ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "mimo-v2-flash"]
    
    for model_name in supported_models:
        try:
            llm = ChatOpenAI(
                model=model_name,
                api_key=api_key,
                base_url=api_base,
                max_tokens=5,
                top_p=0.95,
                timeout=10
            )
            llm.invoke("hi")
            return model_name
        except Exception as e:
            continue
    
    return None

def validate_model_st(config_params):
    """æ¨¡å‹å¯ç”¨æ€§é¢„æ£€ (Streamlit ç‰ˆ) - å¸¦æŒä¹…åŒ–ç¼“å­˜å’Œè‡ªåŠ¨æ¢æµ‹"""
    from langchain_openai import ChatOpenAI
    import hashlib
    
    # 1. ä¼˜å…ˆå°è¯•ç”¨æˆ·å½“å‰é€‰æ‹©çš„æ¨¡å‹
    target_model = config_params.get("model_name")
    if target_model:
        try:
            llm = ChatOpenAI(
                model=target_model,
                api_key=config_params["api_key"],
                base_url=config_params["api_base"],
                max_tokens=5,
                top_p=0.95,
                timeout=10
            )
            llm.invoke("hi")
            return True, "", target_model
        except Exception as e:
            st.warning(f"âš ï¸ é€‰æ‹©çš„æ¨¡å‹ {target_model} éªŒè¯å¤±è´¥ï¼Œæ­£åœ¨å°è¯•ç¼“å­˜æˆ–è‡ªåŠ¨æ¢æµ‹...")

    # 2. å°è¯•ä»æŒä¹…åŒ–ç¼“å­˜åŠ è½½
    cached_model = load_model_cache()
    cache_key = hashlib.md5(
        f"{config_params['api_base']}_{config_params['model_name']}_{config_params['api_key']}".encode()
    ).hexdigest()
    
    # æ£€æŸ¥ session ç¼“å­˜
    if "model_validation_cache" not in st.session_state:
        st.session_state.model_validation_cache = {}
    
    if cache_key in st.session_state.model_validation_cache:
        cached_result = st.session_state.model_validation_cache[cache_key]
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ5åˆ†é’Ÿï¼‰
        if (datetime.now() - cached_result["timestamp"]).total_seconds() < 300:
            return cached_result["is_ok"], cached_result["error"], cached_result["model"]
    
    # æ‰§è¡ŒéªŒè¯
    try:
        llm = ChatOpenAI(
            model=config_params["model_name"], 
            api_key=config_params["api_key"], 
            base_url=config_params["api_base"], 
            max_tokens=5,
            top_p=0.95,
            timeout=10
        )
        llm.invoke("hi")
        result = (True, "", config_params["model_name"])
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–ç¼“å­˜
        save_model_cache(config_params["model_name"])
    except Exception as e:
        # å¦‚æœæŒ‡å®šçš„æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•è‡ªåŠ¨æ¢æµ‹
        st.info(f"ğŸ” æ¨¡å‹ {config_params['model_name']} ä¸å¯ç”¨ï¼Œæ­£åœ¨è‡ªåŠ¨æ¢æµ‹å¯ç”¨æ¨¡å‹...")
        available_model = detect_available_model_st(config_params["api_key"], config_params["api_base"])
        
        if available_model:
            st.success(f"âœ… è‡ªåŠ¨æ¢æµ‹åˆ°å¯ç”¨æ¨¡å‹: {available_model}")
            result = (True, "", available_model)
            
            # ä¿å­˜åˆ°æŒä¹…åŒ–ç¼“å­˜
            save_model_cache(available_model)
        else:
            result = (False, str(e), None)
    
    # ä¿å­˜åˆ° session ç¼“å­˜
    st.session_state.model_validation_cache[cache_key] = {
        "is_ok": result[0],
        "error": result[1],
        "model": result[2],
        "timestamp": datetime.now()
    }
    
    return result

def get_error_solutions(error_msg: str) -> list:
    """
    æ ¹æ®é”™è¯¯ä¿¡æ¯è¿”å›è§£å†³æ–¹æ¡ˆåˆ—è¡¨
    """
    solutions = []
    error_lower = error_msg.lower()
    
    if "400" in error_msg or "model" in error_lower or "not found" in error_lower:
        solutions.extend([
            "ğŸ”§ **æ¨¡å‹ä¸æ”¯æŒ**: è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©å…¶ä»–æ¨¡å‹ï¼Œæˆ–åœ¨ .env æ–‡ä»¶ä¸­é…ç½® SUPPORTED_MODELS",
            "ğŸ”§ **æ£€æŸ¥ API Base**: ç¡®è®¤ API Base URL æ˜¯å¦æ­£ç¡®",
            "ğŸ”§ **æ£€æŸ¥ API Key**: ç¡®è®¤ API Key æ˜¯å¦æœ‰æ•ˆä¸”æœªè¿‡æœŸ"
        ])
    elif "401" in error_msg or "unauthorized" in error_lower or "invalid" in error_lower:
        solutions.extend([
            "ğŸ”§ **API Key æ— æ•ˆ**: è¯·æ£€æŸ¥ä¾§è¾¹æ çš„ API Key æ˜¯å¦æ­£ç¡®",
            "ğŸ”§ **API Key è¿‡æœŸ**: è¯·é‡æ–°è·å–æœ‰æ•ˆçš„ API Key",
            "ğŸ”§ **æƒé™ä¸è¶³**: ç¡®è®¤ API Key æ˜¯å¦æœ‰è®¿é—®è¯¥æ¨¡å‹çš„æƒé™"
        ])
    elif "timeout" in error_lower or "connection" in error_lower:
        solutions.extend([
            "ğŸ”§ **ç½‘ç»œè¿æ¥é—®é¢˜**: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸",
            "ğŸ”§ **API æœåŠ¡ä¸ç¨³å®š**: è¯·ç¨åé‡è¯•",
            "ğŸ”§ **ä»£ç†é—®é¢˜**: å¦‚æœä½¿ç”¨ä»£ç†ï¼Œè¯·æ£€æŸ¥ä»£ç†è®¾ç½®"
        ])
    elif "rate" in error_lower or "limit" in error_lower:
        solutions.extend([
            "ğŸ”§ **è¯·æ±‚é¢‘ç‡é™åˆ¶**: è¯·ç¨åé‡è¯•",
            "ğŸ”§ **é…é¢ä¸è¶³**: è¯·æ£€æŸ¥ API é…é¢æ˜¯å¦å……è¶³"
        ])
    elif "akshare" in error_lower or "no tables found" in error_lower:
        solutions.extend([
            "ğŸ”§ **æ•°æ®æºé—®é¢˜**: AkShare æ•°æ®æºå¯èƒ½æš‚æ—¶ä¸å¯ç”¨",
            "ğŸ”§ **æ¥å£å˜æ›´**: æ•°æ®æ¥å£å¯èƒ½å·²æ›´æ–°ï¼Œè¯·ç¨åé‡è¯•",
            "ğŸ”§ **è‚¡ç¥¨ä»£ç é”™è¯¯**: è¯·ç¡®è®¤è‚¡ç¥¨ä»£ç æ˜¯å¦æ­£ç¡®"
        ])
    else:
        solutions.extend([
            "ğŸ”§ **æœªçŸ¥é”™è¯¯**: è¯·æ£€æŸ¥ç³»ç»Ÿæ—¥å¿—è·å–æ›´å¤šä¿¡æ¯",
            "ğŸ”§ **è”ç³»æ”¯æŒ**: å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ"
        ])
    
    return solutions

def run_workflow(input_str, config_params):
    # 0. å¼ºæ ¡éªŒ API Key
    if not config_params.get("api_key"):
        st.error("âŒ æœªé…ç½® API Keyï¼Œè¯·åœ¨ä¾§è¾¹æ é…ç½®åå†è¯•ã€‚")
        return
    
    # æ¨¡å‹å¯ç”¨æ€§é¢„æ£€
    with st.status("ğŸ§ª æ­£åœ¨éªŒè¯æ¨¡å‹å¯ç”¨æ€§...", expanded=False) as status:
        is_ok, err, available_model = validate_model_st(config_params)
        if not is_ok:
            status.update(label="âŒ æ¨¡å‹éªŒè¯å¤±è´¥", state="error")
            st.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {err}")
            
            # æ˜¾ç¤ºé’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆ
            solutions = get_error_solutions(err)
            if solutions:
                st.info("ğŸ’¡ **å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ**:")
                for solution in solutions:
                    st.markdown(solution)
            return
        
        # æ›´æ–°é…ç½®å‚æ•°ä¸­çš„æ¨¡å‹åç§°
        if available_model:
            config_params["model_name"] = available_model
            st.info(f"âœ… ä½¿ç”¨æ¨¡å‹: {available_model}")
        
        status.update(label="âœ… æ¨¡å‹éªŒè¯é€šè¿‡", state="complete")

    # 1. è¯†åˆ«æ˜¯è‚¡ç¥¨è¿˜æ˜¯æ¿å—
    with st.status("ğŸ” æ­£åœ¨æ£€ç´¢ä¿¡æ¯...", expanded=True) as status:
        stock_code, stock_name, is_sector, sector_type, sector_cons = _get_entity_info(input_str)
            
        if not stock_code:
            st.error(f"æœªæ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨æˆ–æ¿å—: {input_str}")
            return
            
        st.session_state.current_stock = {"code": stock_code, "name": stock_name, "is_sector": is_sector}
        type_str = "æ¿å—" if is_sector else "è‚¡ç¥¨"
        status.update(label=f"âœ… å·²æ‰¾åˆ°{type_str}: {stock_name} ({stock_code})", state="complete")

    # 2. å¯åŠ¨ LangGraph å·¥ä½œæµ
    with st.status("ğŸš€ AlphaFlow å¤šæ™ºèƒ½ä½“åä½œä¸­...", expanded=True) as status:
        st.write("ğŸ“¡ æ­£åœ¨åŒæ­¥å¸‚åœºèµ„è®¯ä¸å®æ—¶æ•°æ®...")
        
        initial_state = {
            "stock_code": stock_code,
            "stock_name": stock_name,
            "is_sector": is_sector,
            "sector_type": sector_type,
            "sector_cons": sector_cons,
            "news_items": [],
            "news_analysis": "",
            "sentiment_score": 0.0,
            "quant_data": {
                "backtest_candidates": []
            },
            "technical_indicators": {},
            "strategy_report": "",
            "risk_assessment": "",
            "messages": [],
            "revision_needed": False,
            "human_approval": False,
            "count": 0,
            "error": "",
            "config": config_params
        }
        
        # 3. è¿è¡Œå›¾
        try:
            # ä½¿ç”¨ stream æ¨¡å¼æ¥æ•è·èŠ‚ç‚¹åˆ‡æ¢
            final_state = initial_state
            for output in st.session_state.app.stream(initial_state):
                for node_name, state_update in output.items():
                    final_state.update(state_update)
                    
                    if node_name == "supervisor":
                        st.write("ğŸš€ **è°ƒåº¦å‘˜**: ä»»åŠ¡åˆ†å‘ä¸­...")
                    elif node_name == "news_node":
                        st.write("ğŸ•µï¸â€â™‚ï¸ **èµ„è®¯ä¾¦å¯Ÿå…µ**: æ·±åº¦æ£€ç´¢ AkShare ä¸“ä¸šèµ„è®¯å®Œæˆ")
                    elif node_name == "quant_node":
                        st.write("ğŸ“Š **æ•°æ®åˆ†æå¸ˆ**: é‡åŒ–æŒ‡æ ‡è®¡ç®—ä¸å¤šç­–ç•¥å›æµ‹å®Œæˆ")
                    elif node_name == "strategy_node":
                        st.write("ğŸ§  **ç­–ç•¥ä¸»ç†äºº**: æ­£åœ¨ç»¼åˆç ”åˆ¤å¹¶ç”ŸæˆæŠ¥å‘Š...")
                    elif node_name == "risk_node":
                        st.write("ğŸ›¡ï¸ **é£æ§å®˜**: æ­£åœ¨å®¡æ ¸æŠ¥å‘Šé€»è¾‘ä¸åˆè§„æ€§...")
            
            status.update(label="âœ… åˆ†æä»»åŠ¡å®Œæˆï¼", state="complete", expanded=False)
            st.session_state.workflow_state = final_state
            
            # 4. å±•ç¤ºç»“æœ
            display_results(final_state)
        except Exception as e:
            st.error(f"âŒ å·¥ä½œæµè¿è¡Œå¤±è´¥: {str(e)}")
            
            # æ˜¾ç¤ºé’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆ
            solutions = get_error_solutions(str(e))
            if solutions:
                st.info("ğŸ’¡ **å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ**:")
                for solution in solutions:
                    st.markdown(solution)

def display_results(state):
    # å°†æŠ¥å‘ŠåŠ å…¥æ¶ˆæ¯å†å²
    report = state.get("strategy_report", "æœªç”ŸæˆæŠ¥å‘Š")
    risk = state.get("risk_assessment", "æœªè¿›è¡Œå®¡æ ¸")
    reasonings = state.get("reasoning_content", [])
    stock_code = state.get("stock_code")
    stock_name = state.get("stock_name")
    is_sector = state.get("is_sector", False)
    
    # 0. å±•ç¤ºç¼“å­˜çŠ¶æ€
    cache_status = get_cache_status(stock_code)
    if cache_status and cache_status.get("data_sources"):
        with st.expander("ğŸ“¦ æ•°æ®ç¼“å­˜çŠ¶æ€"):
            st.info(f"ç¼“å­˜æ–‡ä»¶: {cache_status['cache_file']} | æ€»ç¼“å­˜æ¡ç›®: {cache_status['cache_size']}")
            
            data_sources = cache_status["data_sources"]
            if any(ds.get("last_updated") for ds in data_sources.values()):
                st.write("**å„æ•°æ®æºæœ€åæ›´æ–°æ—¶é—´:**")
                for source_name, source_info in data_sources.items():
                    last_updated = source_info.get("last_updated")
                    if last_updated:
                        try:
                            from datetime import datetime
                            dt = datetime.fromisoformat(last_updated)
                            time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                            st.write(f"- **{source_name}**: {time_str}")
                        except:
                            st.write(f"- **{source_name}**: {last_updated}")
                    else:
                        st.write(f"- **{source_name}**: æœªç¼“å­˜")
            else:
                st.write("æš‚æ— ç¼“å­˜æ•°æ®")
    
    # 1. å±•ç¤ºä»·æ ¼èµ°åŠ¿å›¾
    st.subheader(f"ğŸ“ˆ {stock_name} ({stock_code}) {'æ¿å—' if is_sector else 'è‚¡ç¥¨'}ä»·æ ¼èµ°åŠ¿")
    try:
        if is_sector:
            df = get_board_hist_data(stock_name, board_type=state.get("sector_type", "industry"), days=100)
        else:
            df = get_stock_hist_data(stock_code, days=100)
            
        if isinstance(df, pd.DataFrame) and not df.empty:
            fig = go.Figure(data=[go.Candlestick(x=df['æ—¥æœŸ'] if 'æ—¥æœŸ' in df.columns else df.index,
                            open=df['å¼€ç›˜'],
                            high=df['æœ€é«˜'],
                            low=df['æœ€ä½'],
                            close=df['æ”¶ç›˜'],
                            name='Kçº¿')])
            fig.update_layout(xaxis_rangeslider_visible=False, height=400, margin=dict(l=20, r=20, t=20, b=20))
            st.plotly_chart(fig, width="stretch")
    except Exception as e:
        st.warning(f"æ— æ³•åŠ è½½ K çº¿å›¾: {e}")

    # 2. å¦‚æœæ˜¯æ¿å—ï¼Œå±•ç¤ºæˆåˆ†è‚¡
    if is_sector and state.get("sector_cons"):
        with st.expander("ğŸ”— æŸ¥çœ‹æ¿å—æ ¸å¿ƒæˆåˆ†è‚¡"):
            cons_df = pd.DataFrame(state["sector_cons"])
            st.dataframe(cons_df, width="stretch")

    # 3. å±•ç¤ºæ€è€ƒè¿‡ç¨‹ (å¦‚æœæœ‰ä¸”å¼€å¯äº†æ€è€ƒæ¨¡å¼)
    if reasonings and state.get("config", {}).get("thinking_mode", True):
        with st.expander("ğŸ§  æŸ¥çœ‹ AI æ·±åº¦æ€è€ƒè¿‡ç¨‹"):
            for r in reasonings:
                st.write(f"**{r['agent']}**: ")
                st.info(r['content'])
    
    # 3. å±•ç¤ºåˆ†æç»“è®ºä¸ä¸‹è½½æŒ‰é’®
    
    # å¤„ç†ç»“æ„åŒ–çš„é£æ§ç»“æœ
    if isinstance(risk, dict):
        decision = risk.get("decision", "æœªçŸ¥")
        reason = risk.get("reason", "æœªæä¾›è¯¦ç»†ç†ç”±")
        review_count = risk.get("review_count", 0)
        review_date = risk.get("review_date", "")
        
        risk_text = f"### ï¿½ï¸ é£æ§æ„è§\n\nã€å†³ç­–: {decision}ã€‘\nã€å®¡æ ¸æ¬¡æ•°: {review_count}ã€‘"
        if review_date:
            risk_text += f"\nã€å®¡æ ¸æ—¥æœŸ: {review_date}ã€‘"
        risk_text += f"\n\n{reason}"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼ºåˆ¶é€šè¿‡
        if decision == "å¼ºåˆ¶é€šè¿‡":
            risk_text += "\n\n---\n\nâš ï¸ **é‡è¦æç¤º**: è¯¥æŠ¥å‘Šå·²è¾¾åˆ°æœ€å¤§ä¿®è®¢æ¬¡æ•°ï¼Œç³»ç»Ÿå¼ºåˆ¶é€šè¿‡ã€‚\n\nâš ï¸ **é£é™©æç¤º**: è¯·ä»”ç»†é˜…è¯»é£æ§å®˜çš„æœ«æ¬¡é£é™©æç¤ºï¼Œå»ºè®®äººå·¥å¤æ ¸åå†åšæŠ•èµ„å†³ç­–ã€‚"
    else:
        # å…¼å®¹æ—§æ ¼å¼ï¼ˆå­—ç¬¦ä¸²ï¼‰
        risk_text = f"### ğŸ›¡ï¸ é£æ§æ„è§\n\n{risk}"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼ºåˆ¶é€šè¿‡
        if "å¼ºåˆ¶é€šè¿‡" in risk:
            risk_text += "\n\n---\n\nâš ï¸ **é‡è¦æç¤º**: è¯¥æŠ¥å‘Šå·²è¾¾åˆ°æœ€å¤§ä¿®è®¢æ¬¡æ•°ï¼Œç³»ç»Ÿå¼ºåˆ¶é€šè¿‡ã€‚\n\nâš ï¸ **é£é™©æç¤º**: è¯·ä»”ç»†é˜…è¯»é£æ§å®˜çš„æœ«æ¬¡é£é™©æç¤ºï¼Œå»ºè®®äººå·¥å¤æ ¸åå†åšæŠ•èµ„å†³ç­–ã€‚"
    
    full_content = f"### ğŸ“‹ æŠ•èµ„å»ºè®®æŠ¥å‘Š\n{report}\n\n---\n{risk_text}"
    
    col1, col2 = st.columns([0.8, 0.2])
    with col1:
        st.markdown(full_content)
    with col2:
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æŠ¥å‘Š",
            data=full_content,
            file_name=f"{stock_name}_{stock_code}_æŠ•èµ„å»ºè®®.md",
            mime="text/markdown"
        )
    
    # 4. å±•ç¤ºå›æµ‹å€™é€‰ç­–ç•¥
    with st.expander("ğŸ“ˆ å›æµ‹å€™é€‰ç­–ç•¥è¯¦æƒ…"):
        quant_data = state.get("quant_data", {})
        candidates = quant_data.get("backtest_candidates", [])
        if candidates:
            st.write(f"**å›æµ‹ç³»ç»Ÿåœ¨ STRATEGY_REGISTRY ä¸­å‘ç°äº† {len(candidates)} ä¸ªå€™é€‰ç­–ç•¥**")
            for i, cand in enumerate(candidates):
                metrics = cand.get('metrics', {})
                with st.container():
                    st.write(f"### {i+1}. {cand.get('name')}")
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Sharpe Ratio", f"{metrics.get('sharpe', 0):.2f}")
                    col2.metric("CAGR (å¹´åŒ–æ”¶ç›Š)", f"{metrics.get('cagr', 0)*100:.2f}%")
                    col3.metric("Max Drawdown", f"{metrics.get('max_drawdown', 0)*100:.2f}%")
                    col4.metric("Win Rate", f"{metrics.get('win_rate', 0)*100:.2f}%")
                    
                    st.write("**ç­–ç•¥æ‘˜è¦:**")
                    st.info(cand.get('summary', 'æš‚æ— æ‘˜è¦'))
                    if i < len(candidates) - 1:
                        st.divider()
        else:
            st.info("æš‚æ— å€™é€‰ç­–ç•¥å›æµ‹æ•°æ®")

    with st.expander("ğŸ“Š æŸ¥çœ‹é‡åŒ–ä¸è´¢åŠ¡æ•°æ®åº•ç¨¿"):
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            st.write("**æŠ€æœ¯é¢ä¸å½¢æ€è¯†åˆ«**")
            tech = state.get("technical_indicators", {})
            patterns = tech.get("identified_patterns", [])
            if patterns:
                st.success(f"è¯†åˆ«å½¢æ€: {', '.join(patterns)}")
            st.json(tech)
        with col_d2:
            st.write("**è´¢åŠ¡ã€è¡Œä¸šå¯¹æ¯”ä¸èµ„é‡‘é¢**")
            # æ’é™¤ backtest_candidates ä»¥å…å†—ä½™
            display_quant = {k: v for k, v in state.get("quant_data", {}).items() if k != "backtest_candidates"}
            st.json(display_quant)
            
    with st.expander("ğŸ“° æŸ¥çœ‹æœ€æ–°èµ„è®¯åŸæ–‡"):
        news = state.get("news_items", [])
        if isinstance(news, list) and news:
            for item in news:
                st.write(f"- **{item.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜')}** ({item.get('å‘å¸ƒæ—¶é—´', 'æœªçŸ¥æ—¶é—´')})")
                st.caption(item.get('æ–°é—»å†…å®¹', 'æ— å†…å®¹')[:200] + "...")
        else:
            st.write("æš‚æ— èµ„è®¯æ•°æ®")
    
    if state.get("news_analysis"):
        with st.expander("ğŸ“ èµ„è®¯åˆ†ææ‘˜è¦"):
            st.write(state["news_analysis"])

    st.session_state.messages.append({"role": "assistant", "content": full_content})
    # ä¿å­˜å†å²è®°å½•
    save_history(stock_name, stock_code, full_content)
    st.rerun()

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥è‚¡ç¥¨ä»£ç æˆ–åç§° (å¦‚: è´µå·èŒ…å°)", disabled=not api_key):
    # å¼€å¯æ–°çš„æœç´¢
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    # æ„é€ é…ç½®å‚æ•°
    config_params = {
        "model_name": model_to_use,
        "api_base": api_base,
        "api_key": api_key if api_key else os.getenv("OPENAI_API_KEY"),
        "temperature": temperature,
        "max_tokens": max_tokens,
        "thinking_mode": thinking_mode
    }
    run_workflow(prompt, config_params)

if not api_key:
    st.info("ğŸ’¡ è¯·åœ¨ä¾§è¾¹æ å¡«å†™ API Key åå¼€å§‹åˆ†æã€‚")

